# generator_bert.py
# ì‹¤í–‰ë°©ë²•: python generator_bert.py

import pandas as pd
import numpy as np
import os
import json
import torch
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from typing import Dict, List, Tuple

# --- í™˜ê²½ ì„¤ì • ---
# DATA_DIR = "dataSet"
DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'dataSet')

# í•œêµ­ì–´ BERT ëª¨ë¸ (KoBERT) ë¡œë“œ
MODEL_NAME = "skt/kobert-base-v1" 

# Streamlit í™˜ê²½ì—ì„œ ìºì‹± ê°€ëŠ¥í•˜ë„ë¡ ë³„ë„ í•¨ìˆ˜ë¡œ ì •ì˜
def load_bert_model():
    """BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. (app.pyì—ì„œ ìºì‹±í•˜ì—¬ ì‚¬ìš©)"""
    try:
        from kobert_transformers import get_tokenizer as get_kobert_tokenizer # í•¨ìˆ˜ ë‚´ì—ì„œ ë‹¤ì‹œ import
        tokenizer = get_kobert_tokenizer()
        model = AutoModel.from_pretrained("skt/kobert-base-v1")
        model.eval()
        return tokenizer, model
    except Exception as e:
        # Streamlit í™˜ê²½ì´ ì•„ë‹ˆë¯€ë¡œ ì˜¤ë¥˜ë¥¼ ë°˜í™˜
        print(f"BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}") 
        return None, None
    
# ìµœì¢… ì „ì²˜ë¦¬ ë° ì‚¬ìš©ìë³„ ì„±í–¥ ë²¡í„° ì§‘ê³„
def aggregate_user_profiles(df: pd.DataFrame, min_playtime_hours: int = 2) -> Tuple[Dict[str, float], List[str]]:
    """
    ê°œë³„ ë¦¬ë·° ë°ì´í„°ë¥¼ ì‚¬ìš©ì(author_id)ë³„ ìµœì¢… ì„±í–¥ ë²¡í„°ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤.
    """
    # try:
    #     df = pd.read_csv(input_csv_path)
    # except FileNotFoundError:
    #     print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_csv_path}")
    #     return None
    
    df['playtime_hours'] = df['playtime_forever'] / 60
    
    # 1) ì‹ ë¢°ë„ ë‚®ì€ ë°ì´í„° í•„í„°ë§ (í”Œë ˆì´ ì‹œê°„ 2ì‹œê°„ ë¯¸ë§Œ(ê²Œì„í™˜ë¶ˆì‹œê°„:2h), í…ìŠ¤íŠ¸ ì—†ìŒ)
    df_filtered = df[
        (df['playtime_hours'] >= min_playtime_hours) & 
        (df['review_text'].str.strip() != '') &
        (df['review_text'].notna())
    ].copy()

    if df_filtered.empty:
        # print("ê²½ê³ : í•„í„°ë§ í›„ ë¶„ì„í•  ìœ íš¨í•œ ë¦¬ë·°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return {}, []
    
    # print(f"âœ… ìµœì¢… ë¶„ì„ ëŒ€ìƒ ë¦¬ë·° ìˆ˜: {len(df_filtered)}ê°œ")
    
    # ì„±í–¥ ë²¡í„° ì»¬ëŸ¼ ëª©ë¡
    persona_cols = [col for col in df_filtered.columns if col.startswith('S_')]
    
    # 2) ì‚¬ìš©ìë³„ ìµœì¢… ì„±í–¥ ë²¡í„° ë° í†µê³„ ì§‘ê³„
    agg_args = {
        'review_text': lambda x: x.loc[x.str.len().idxmax()] if x.str.len().max() > 0 else '',
        'voted_up': 'mean',
        'playtime_forever': 'mean'
    }
    for col in persona_cols:
        agg_args[col] = 'mean'
    
    agg_df = df_filtered.groupby('author_id').agg(agg_args).reset_index()
    
    # ì „ì²´ ìœ ì €ì˜ ìµœì¢… í‰ê·  ì„±í–¥ ë²¡í„° (ìš”ì•½ ë° íƒœê·¸ ìƒì„±ì— ì‚¬ìš©)
    game_persona_vector = agg_df[persona_cols].mean().to_dict()
    
    # # ê²Œì„ ì´ë¦„ ì¶”ì¶œ
    # # ì˜ˆ: analyzed_ì‚°ë‚˜ë¹„_reviews.csv
    # parts = os.path.basename(input_csv_path).split('_')
    # game_name = parts[-2]
    
    # ëª¨ë“  ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ (ìš”ì•½ì— ì‚¬ìš©)
    all_reviews = df['review_text'].fillna('').tolist()
    
    return game_persona_vector, all_reviews

# BERT ê¸°ë°˜ ì¶”ì¶œ ìš”ì•½ (Extractive Summarization)
def get_sentence_embeddings(reviews: List[str], tokenizer: AutoTokenizer, model: AutoModel) -> Tuple[List[str], np.ndarray | None]:
    """ë¬¸ì¥ì„ BERT ëª¨ë¸ì„ í†µí•´ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    # ëª¨ë“  ë¦¬ë·°ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    sentences = []
    for review in reviews:
        if review is None or not isinstance(review, str):
            continue
        # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ ê¸°ì¤€)
        sentences.extend([s.strip() for s in review.split('.') if s.strip()])

    # ë¬¸ì¥ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ë©”ëª¨ë¦¬ ë¬¸ì œ ë°œìƒ ê°€ëŠ¥, ìµœëŒ€ 500ê°œ ë¬¸ì¥ë§Œ ì‚¬ìš©
    sentences = sentences[:500] 
    
    if not sentences:
        return [], None
    
    # í† í°í™” ë° ì„ë² ë”© ìƒì„±
    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
        # [CLS] í† í°ì˜ ì¶œë ¥ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
        embeddings = outputs.last_hidden_state[:, 0, :].numpy() 
        
    return sentences, embeddings

def generate_summary(sentences: List[str], embeddings: np.ndarray | None, summary_length: int = 4) -> str:
    """K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ëŒ€í‘œì ì¸ ë¬¸ì¥ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    if not sentences or len(sentences) < summary_length:
        return "ë¦¬ë·° í…ìŠ¤íŠ¸ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ë„ˆë¬´ ì§§ì•„ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    try:
        # K-Means í´ëŸ¬ìŠ¤í„°ë§ (ìš”ì•½ ë¬¸ì¥ ìˆ˜ = í´ëŸ¬ìŠ¤í„° ìˆ˜)
        kmeans = KMeans(n_clusters=summary_length, random_state=42, n_init='auto')
        kmeans.fit(embeddings)
        
        # ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬(Centroid)ì„ ì°¾ìŠµë‹ˆë‹¤.
        centroids = kmeans.cluster_centers_
        
        # ê° í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ì„ ì°¾ìŠµë‹ˆë‹¤. (ëŒ€í‘œ ë¬¸ì¥)
        summary_sentences = []
        
        for i in range(summary_length):
            # í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ” ì„ë² ë”© ë²¡í„°ì™€ ì¤‘ì‹¬ ë²¡í„° ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
            distances = cosine_similarity([centroids[i]], embeddings)[0]
            
            # ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš´ ë¬¸ì¥ì˜ ì¸ë±ìŠ¤ (ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì¥)
            closest_index = np.argsort(distances)[-1] 
            
            # ì¤‘ë³µ ë°©ì§€
            if sentences[closest_index] not in summary_sentences:
                summary_sentences.append(sentences[closest_index])
                
        return ". ".join(summary_sentences) + "."
    except Exception as e:
            return f"ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ì„±í–¥ ë²¡í„° ê¸°ë°˜ íƒœê·¸ ì˜ˆì¸¡
def predict_tags(persona_vector: Dict[str, float]) -> List[str]:
    """ì„±í–¥ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œ íƒœê·¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì„ê³„ê°’
    THRESHOLD = 0.15 
    
    # ë¯¸ë¦¬ ì •ì˜ëœ íƒœê·¸ í›„ë³´ ëª©ë¡
    TAG_CANDIDATES = {
        'narrative': ["#ê°“ì„œì‚¬", "#ìŠ¤í† ë¦¬_ì¤‘ì‹¬", "#ë›°ì–´ë‚œ_ì—°ì¶œ"],
        'freedom': ["#ë†’ì€_ììœ ë„", "#íƒí—˜_í•„ìˆ˜", "#ë‚˜ë§Œì˜_ì„ íƒ", "#ì˜¤í”ˆì›”ë“œ"],
        'stability': ["#ê°“ì í™”", "#ì¾Œì í•œ_í™˜ê²½", "#ë²„ê·¸_ì—†ìŒ", "#ì•ˆì •ì "],
        'challenge': ["#ê·¹ì•…ì˜_ë‚œì´ë„", "#ë„ì „ê³¼ì œ", "#í”¼ì§€ì»¬_ìš”êµ¬", "#ê³ ìˆ˜ì „ìš©"]
    }
    
    tag_scores = {}
    # 1. 1ì°¨ ì‹œë„: ì„ê³„ê°’ 0.15 ì´ìƒì¸ íƒœê·¸ë§Œ ìˆ˜ì§‘
    for vector_key, score in persona_vector.items():
        key_without_s = vector_key.replace('S_', '')
        if key_without_s in TAG_CANDIDATES and score >= THRESHOLD:
            for tag in TAG_CANDIDATES[key_without_s]:
                tag_scores[tag] = tag_scores.get(tag, 0) + score
    
    # 2. 2ì°¨ ì‹œë„: ì„ê³„ê°’ 0.15ë¥¼ ë„˜ì§€ ëª»í–ˆë‹¤ë©´, ìµœê³ ì ì„ ê°€ì§„ ì¶• 1ê°œë§Œ ì„ íƒ (í´ë°±)
    if not tag_scores:
        # ë²¡í„° ê°’ì´ ê°€ì¥ ë†’ì€ ì¶•ê³¼ ê·¸ ê°’ ì°¾ê¸°
        best_axis = max(persona_vector, key=persona_vector.get)
        best_score = persona_vector[best_axis]
            
        # ìµœê³ ì  ì¶•ì˜ ì²« ë²ˆì§¸ íƒœê·¸ë§Œ ì„ íƒ
        key_without_s = best_axis.replace('S_', '')
        if key_without_s in TAG_CANDIDATES:
            # í•´ë‹¹ ì¶•ì˜ ëŒ€í‘œ íƒœê·¸ 1ê°œë§Œ ë°˜í™˜
            return [TAG_CANDIDATES[key_without_s][0]]
                    
    # ì ìˆ˜ê°€ ë†’ì€ ìƒìœ„ 5ê°œ íƒœê·¸ ì„ íƒ
    sorted_tags = sorted(tag_scores.items(), key=lambda item: item[1], reverse=True)[:5]
    
    # íƒœê·¸ ëª©ë¡ë§Œ ë°˜í™˜
    return [tag for tag, score in sorted_tags]

def run_bert_generation(analyzed_csv_path: str, game_name: str, tokenizer: AutoTokenizer, model: AutoModel) -> Tuple[str, List[str], str, float]:
    """
    ë¶„ì„ëœ CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ BERT ìƒì„± ë° TXT íŒŒì¼ ì €ì¥ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    try:
        df = pd.read_csv(analyzed_csv_path)
    except Exception as e:
        raise FileNotFoundError(f"ë¶„ì„ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {analyzed_csv_path}. ì˜¤ë¥˜: {e}")
    
    if df.empty:
        raise ValueError("ë¡œë“œëœ ë¶„ì„ ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¦¬ë·° ìˆ˜ì§‘ì„ í™•ì¸í•˜ì„¸ìš”.")
    
    # 1.1. ì „ì²´ ê¸ì • ë¹„ìœ¨ ê³„ì‚° (ëª¨ë“  ë¦¬ë·° ì‚¬ìš©)
    total_reviews = len(df)
    # 'voted_up' (True/False)ë¥¼ 1/0ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•©ì‚°
    df['voted_up'] = df['voted_up'].astype(int)
    positive_count = df['voted_up'].sum() 
    pos_ratio = round((positive_count / total_reviews) * 100, 1) if total_reviews > 0 else 0.0

    # 1.2. ìµœì¢… ì „ì²˜ë¦¬ ë° ì‚¬ìš©ìë³„ ì„±í–¥ ë²¡í„° ì§‘ê³„
    game_persona_vector, all_reviews = aggregate_user_profiles(df)
    
    if not game_persona_vector:
        # ìœ íš¨í•œ ë¦¬ë·°ê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
        safe_game_name = game_name.replace(' ', '_')
        output_filename = os.path.join(DATA_DIR, f"BERT_Analysis_{safe_game_name}.txt")
        
        with open(output_filename, 'w', encoding='utf-8') as f:
            f.write(f"ê²Œì„: {game_name}\n")
            f.write(f"ê¸ì • ë¹„ìœ¨: {pos_ratio}%\n\n")  # ğŸ’¡ ê¸ì • ë¹„ìœ¨ ì¶”ê°€
            f.write("ìš”ì•½:\nìœ íš¨í•œ ë¦¬ë·°(í”Œë ˆì´ ì‹œê°„ 5ì‹œê°„ ì´ìƒ)ê°€ ë¶€ì¡±í•˜ì—¬ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.\n\n")
            f.write("ì¶”ì²œ íƒœê·¸:\n\nì„±í–¥ ë²¡í„°:\n{}")
            
        return "ë¶„ì„í•  ìœ íš¨í•œ ë¦¬ë·°ê°€ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", [], ""
    
    # 2. ìš”ì•½ ìƒì„± (BERT)
    sentences, embeddings = get_sentence_embeddings(all_reviews, tokenizer, model)
    summary = generate_summary(sentences, embeddings, summary_length=4)
    
    # 3. íƒœê·¸ ìƒì„± (ì„±í–¥ ë²¡í„° ê¸°ë°˜)
    predicted_tags = predict_tags(game_persona_vector)
    
    # 4. ê²°ê³¼ ì €ì¥
    safe_game_name = game_name.replace(' ', '_')
    output_filename = os.path.join(DATA_DIR, f"BERT_Analysis_{safe_game_name}.txt")
    
    with open(output_filename, 'w', encoding='utf-8') as f:
        f.write(f"ê²Œì„: {game_name}\n")
        f.write(f"ê¸ì • ë¹„ìœ¨: {pos_ratio}%\n\n")
        f.write(f"ìš”ì•½:\n{summary}\n\n")
        f.write(f"ì¶”ì²œ íƒœê·¸:\n{', '.join(predicted_tags)}\n\n")
        f.write(f"ì„±í–¥ ë²¡í„°:\n{json.dumps(game_persona_vector, ensure_ascii=False, indent=4)}\n")
    
    return summary, predicted_tags, output_filename, pos_ratio

# def main_generator_bert():
#     # íŒŒì¼ ê²½ë¡œ ì„¤ì • (dataSet ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì„ ë¡œë“œ)
#     input_filename = input("ë¶„ì„ëœ CSV íŒŒì¼ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: analyzed_{appID}_reviews.csv): ")
#     input_csv_path = os.path.join(DATA_DIR, input_filename)

#     # 1. ìµœì¢… ì „ì²˜ë¦¬ ë° ì„±í–¥ ë²¡í„° ì§‘ê³„
#     result = aggregate_user_profiles(input_csv_path)
    
#     if result is None:
#         print("ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ ì‚¬ìš©ì í”„ë¡œí•„ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
#         return

#     game_name, persona_vector, all_reviews = result
    
#     # 2. BERT ëª¨ë¸ ë¡œë“œ
#     print(f"\n! {MODEL_NAME} ëª¨ë¸ ë¡œë“œ ì¤‘...")
#     try:
#         tokenizer = get_kobert_tokenizer()
#         model = AutoModel.from_pretrained(MODEL_NAME)
#         model.eval() # ì¶”ë¡  ëª¨ë“œ
#     except Exception as e:
#         print(f"âŒ BERT ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
#         return

#     # 3. ìš”ì•½ ìƒì„± (BERT ê¸°ë°˜ ì¶”ì¶œ ìš”ì•½)
#     sentences, embeddings = get_sentence_embeddings(all_reviews, tokenizer, model)
#     summary = generate_summary(sentences, embeddings, summary_length=4)
    
#     # 4. íƒœê·¸ ìƒì„± (ì„±í–¥ ë²¡í„° ê¸°ë°˜ ì˜ˆì¸¡)
#     predicted_tags = predict_tags(persona_vector)
    
#     # 5. ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
#     print("\n========================================")
#     print(f"ğŸ® ê²Œì„ ë¶„ì„ ê²°ê³¼: {game_name}")
#     print("========================================")
#     print("1. ì¶”ì¶œ ìš”ì•½:")
#     print(summary)
#     print("\n2. ì„±í–¥ ë²¡í„° ê¸°ë°˜ ì¶”ì²œ íƒœê·¸:")
#     print(f"{', '.join(predicted_tags)}")
#     print("\n3. ìµœì¢… ì„±í–¥ ë²¡í„°:")
#     print(json.dumps({k.replace('S_', ''): round(v, 4) for k, v in persona_vector.items()}, indent=4, ensure_ascii=False))
#     print("========================================")
    
#     # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
#     output_filename = os.path.join(DATA_DIR, f"BERT_Analysis_{game_name}.txt")
#     with open(output_filename, 'w', encoding='utf-8') as f:
#         f.write(f"ê²Œì„: {game_name}\n")
#         f.write(f"ìš”ì•½:\n{summary}\n\n")
#         f.write(f"ì¶”ì²œ íƒœê·¸:\n{', '.join(predicted_tags)}\n\n")
#         f.write(f"ì„±í–¥ ë²¡í„°:\n{json.dumps(persona_vector, ensure_ascii=False, indent=4)}\n")
    
#     print(f"\nğŸ“‚ ë¶„ì„ ê²°ê³¼ê°€ '{output_filename}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


# if __name__ == "__main__":
#     main_generator_bert()