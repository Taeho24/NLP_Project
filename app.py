# app.py
# ì‹¤í–‰ë°©ë²•: streamlit run app.py

import streamlit as st
import pandas as pd
import numpy as np
import os
import re
import sys
import torch
import json
from transformers import AutoModel
from kobert_transformers import get_tokenizer as get_kobert_tokenizer
from typing import Dict, List, Tuple, Any

# --- ëª¨ë“ˆ ê²½ë¡œ ì„¤ì • ---
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(os.path.join(ROOT_DIR, 'collectData'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))

try:
    from collector import run_collection, search_games
    from analyzer import run_analysis
    from generator_bert import run_bert_generation, load_bert_model
except ImportError as e:
    st.error(f"âŒ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨. collectData/ ë˜ëŠ” model/ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì™€ íŒŒì¼ëª…ì„ í™•ì¸í•˜ì„¸ìš”: {e}")
    sys.exit()

# --- í™˜ê²½ ì„¤ì • ë° ìƒìˆ˜ ---
DATA_DIR = "dataSet"
PERSONA_AXES = ['narrative', 'freedom', 'stability', 'challenge']
PERSONA_LABELS_KO = {
    'narrative': 'ìŠ¤í† ë¦¬/ì„œì‚¬ ì„ í˜¸',
    'freedom': 'ììœ ë„/íƒí—˜ ì„ í˜¸',
    'stability': 'ìµœì í™”/ì•ˆì •ì„± ì„ í˜¸',
    'challenge': 'ë„ì „/ë‚œì´ë„ ì„ í˜¸'
}

@st.cache_resource
def load_bert_resources():
    """BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤."""
    return load_bert_model()

def get_personalized_recommendation(df: pd.DataFrame, user_persona_vector: Dict[str, int]) -> Tuple[str, List[str], int, int]:
    """
    ì‚¬ìš©ì ì„±í–¥ ë²¡í„°ì™€ ê° ë¦¬ë·°ì˜ ì„±í–¥ ë²¡í„°ë¥¼ ë¹„êµí•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œ íƒœê·¸ ë° ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    user_vector = np.array(list(user_persona_vector.values()))
    if np.sum(user_vector) > 0:
        user_vector = user_vector / np.sum(user_vector)
    
    # ì„±í–¥ ë²¡í„° ì¹¼ëŸ¼ëª…ì€ 'S_narrative', 'S_freedom' í˜•íƒœì„
    review_vectors = df[[f'S_{axis}' for axis in PERSONA_AXES]].values
    
    # ê°œì¸í™” ì ìˆ˜ ê³„ì‚° (Dot Product)
    df['personalized_score'] = np.dot(review_vectors, user_vector)
    
    top_n = 10
    personalized_df = df.sort_values(by='personalized_score', ascending=False).head(top_n)
    
    # ìƒìœ„ ë¦¬ë·° í…ìŠ¤íŠ¸ ê²°í•© (ê°œì¸í™” ìš”ì•½)
    top_reviews_text = " ".join(personalized_df['review_text'].tolist())
    
    # ê°œì¸í™”ëœ ë¦¬ë·° 10ê°œì˜ ê¸ì •/ë¶€ì • ê°œìˆ˜ ê³„ì‚°
    pos_count = personalized_df['voted_up'].sum()
    neg_count = top_n - pos_count
    
    # ê°œì¸í™” íƒœê·¸ ì¶”ì¶œ (ì‚¬ìš©ìê°€ ê°•í•˜ê²Œ ì„ í˜¸í•˜ëŠ” ì„±í–¥ ê¸°ë°˜)
    all_keywords = []
    # ìŠ¬ë¼ì´ë” 10ì  ë§Œì  ì¤‘ 7ì  ì´ìƒì„ ê°•í•œ ì„ í˜¸ë„ë¡œ ê°„ì£¼
    for axis in PERSONA_AXES:
        if user_persona_vector[axis] >= 7: 
            TAG_CANDIDATES = {
                'narrative': ["#ê°“ì„œì‚¬", "#ìŠ¤í† ë¦¬_ëª°ì…", "#ê°ë™ì "],
                'freedom': ["#ë†’ì€_ììœ ë„", "#íƒí—˜", "#ë‚˜ë§Œì˜_ì„ íƒ"],
                'stability': ["#ê°“ì í™”", "#ë²„ê·¸ì—†ìŒ", "#ì¾Œì í•¨"],
                'challenge': ["#í•µì‹¬_ë‚œì´ë„", "#ë„ì „ì˜ì‹", "#í”¼ì§€ì»¬_ê²Œì„"]
            }
            all_keywords.extend(TAG_CANDIDATES.get(axis, []))

    return top_reviews_text, list(set(all_keywords)), pos_count, neg_count

def load_summary_data(summary_txt_path: str) -> Dict[str, Any]:
    """ë¶„ì„ TXT íŒŒì¼ì„ ì½ì–´ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    
    # ê¸°ë³¸ê°’ ì„¤ì • (íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì´ ë©”ì‹œì§€ê°€ ì¶œë ¥ë©ë‹ˆë‹¤)
    summary_data = {
        'positive_ratio': None,
        'summary': 'BERT ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
        'tags': [],
        'persona_vector': {}
    }

    try:
        with open(summary_txt_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # 1. ê¸ì • ë¹„ìœ¨ ì¶”ì¶œ
        ratio_match = re.search(r"ê¸ì • ë¹„ìœ¨: (\d+\.?\d*)%", content)
        if ratio_match:
            summary_data['positive_ratio'] = float(ratio_match.group(1)) / 100.0

        # 2. ìš”ì•½ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìš”ì•½:ê³¼ \n\nì¶”ì²œ íƒœê·¸: ì‚¬ì´)
        # re.DOTALLì„ ì‚¬ìš©í•˜ì—¬ ì¤„ë°”ê¿ˆ í¬í•¨ ëª¨ë“  ë¬¸ì ë§¤ì¹­
        summary_match = re.search(r"ìš”ì•½:\s*\n(.*?)\n\nì¶”ì²œ íƒœê·¸:", content, re.DOTALL)
        if summary_match:
            summary_data['summary'] = summary_match.group(1).strip()
            
        # 3. ì¶”ì²œ íƒœê·¸ ì¶”ì¶œ (ì¶”ì²œ íƒœê·¸:ì™€ \n\nì„±í–¥ ë²¡í„°: ì‚¬ì´)
        tag_match = re.search(r"ì¶”ì²œ íƒœê·¸:\s*\n(.*?)\n\nì„±í–¥ ë²¡í„°:", content, re.DOTALL)
        if tag_match:
            raw_tags = tag_match.group(1).strip()
            # ì‰¼í‘œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ í›„, ê° íƒœê·¸ì—ì„œ ê³µë°± ì œê±°
            summary_data['tags'] = [t.strip() for t in raw_tags.split(',') if t.strip()]
        
        # 4. ì„±í–¥ ë²¡í„° ì¶”ì¶œ (ì„±í–¥ ë²¡í„°: ë‹¤ìŒì— ì˜¤ëŠ” JSON ë¸”ë¡)
        # ì„±í–¥ ë²¡í„°: ë‹¤ìŒì— ì˜¤ëŠ” ëª¨ë“  ë‚´ìš©ì„ JSON ë¬¸ìì—´ë¡œ ê°„ì£¼
        vector_match = re.search(r"ì„±í–¥ ë²¡í„°:\s*\n(.*?)\s*$", content, re.DOTALL)
        if vector_match:
            try:
                json_string = vector_match.group(1).strip()
                summary_data['persona_vector'] = json.loads(json_string)
            except json.JSONDecodeError as e:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë””ë²„ê¹… ë©”ì‹œì§€ ì¶œë ¥
                print(f"ì„±í–¥ ë²¡í„° JSON íŒŒì‹± ì˜¤ë¥˜: {e}") 
                summary_data['persona_vector'] = {}

    except FileNotFoundError:
        summary_data['summary'] = f"ë¶„ì„ ê²°ê³¼ íŒŒì¼ ({summary_txt_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    except Exception as e:
        summary_data['summary'] = f"ë¶„ì„ ìš”ì•½ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}"
        print(f"ë¶„ì„ ìš”ì•½ íŒŒì¼ ë¡œë“œ/íŒŒì‹± ì˜¤ë¥˜: {e}")

    return summary_data

# # ë°ì´í„° ë¡œë“œ ë° ìºì‹±
# @st.cache_data
# def load_data(filename):
#     """ë¶„ì„ëœ CSV íŒŒì¼ê³¼ BERT ë¶„ì„ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    
#     # 1-1. ë¶„ì„ëœ ë¦¬ë·° ë°ì´í„° (CSV) ë¡œë“œ
#     csv_path = os.path.join(DATA_DIR, f"analyzed_{filename}_reviews.csv")
#     try:
#         df = pd.read_csv(csv_path)
#     except FileNotFoundError:
#         return None, None, f"âŒ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}"

#     # 1-2. BERT ë¶„ì„ ê²°ê³¼ (TXT) ë¡œë“œ
#     txt_path = os.path.join(DATA_DIR, f"BERT_Analysis_{filename}.txt")
#     bert_summary = {}
#     try:
#         with open(txt_path, 'r', encoding='utf-8') as f:
#             content = f.read()
            
#         # í…ìŠ¤íŠ¸ íŒŒì¼ ë‚´ìš© íŒŒì‹± (ê°„ë‹¨í•˜ê²Œ ìš”ì•½ê³¼ íƒœê·¸ë§Œ ì¶”ì¶œ)
#         summary_match = re.search(r"ìš”ì•½:\n(.*?)\n\n", content, re.DOTALL)
#         tag_match = re.search(r"ì¶”ì²œ íƒœê·¸:\n(.*?)\n\n", content, re.DOTALL)
        
#         bert_summary['summary'] = summary_match.group(1).strip() if summary_match else "BERT ìš”ì•½ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#         bert_summary['tags'] = [t.strip() for t in tag_match.group(1).split(',') if tag_match] if tag_match else []

#     except FileNotFoundError:
#         return df, None, f"âš ï¸ BERT ë¶„ì„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {txt_path}"
    
#     return df, bert_summary, None

# # ê°œì¸í™” ë¡œì§: ì‚¬ìš©ì ì„ í˜¸ë„ ê¸°ë°˜ ë¦¬ë·° ì ìˆ˜ ê³„ì‚°
# def get_personalized_recommendation(df, user_persona_vector):
#     """
#     ì‚¬ìš©ì ì„±í–¥ ë²¡í„°ì™€ ê° ë¦¬ë·°ì˜ ì„±í–¥ ë²¡í„°ë¥¼ ë¹„êµí•˜ì—¬ ì ìˆ˜í™”í•˜ê³  ê°œì¸í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
#     """
#     # 1. ì‚¬ìš©ì ë²¡í„° ì •ê·œí™” (ì´í•© 1)
#     user_vector = np.array(list(user_persona_vector.values()))
#     if np.sum(user_vector) > 0:
#         user_vector = user_vector / np.sum(user_vector)
    
#     # 2. ë¦¬ë·°ë³„ ì„±í–¥ ë²¡í„° ì¶”ì¶œ
#     review_vectors = df[[f'S_{axis}' for axis in PERSONA_AXES]].values
    
#     # 3. ê°œì¸í™” ì ìˆ˜ ê³„ì‚° (Dot Product)
#     # ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ë¦¬ë·°ì˜ ì„±í–¥ ì¼ì¹˜ ì •ë„ë¥¼ ì ìˆ˜í™”
#     df['personalized_score'] = np.dot(review_vectors, user_vector)
    
#     # 4. ê°€ì¥ ì„ í˜¸ë„ê°€ ë†’ì€ ìƒìœ„ 10ê°œ ë¦¬ë·° ì¶”ì¶œ
#     top_n = 10
#     personalized_df = df.sort_values(by='personalized_score', ascending=False).head(top_n)
    
#     # 5. ê°œì¸í™”ëœ ìš”ì•½ ìƒì„± (ì„ í˜¸ë„ ë†’ì€ ë¦¬ë·°ì˜ í‚¤ì›Œë“œ ê¸°ë°˜)
#     top_reviews_text = " ".join(personalized_df['review_text'].tolist())
    
#     # 6. ê°œì¸í™” íƒœê·¸ ì¶”ì¶œ (ì„ í˜¸ë„ê°€ ë†’ì€ ë¦¬ë·°ì—ì„œ ìì£¼ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ ê¸°ë°˜)
#     # ì´ ë¶€ë¶„ì€ BERTì˜ í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ì„ ëŒ€ì²´í•˜ëŠ” ë‹¨ìˆœ í‚¤ì›Œë“œ ì¹´ìš´íŒ…ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
    
#     all_keywords = []
#     # ì˜ˆì‹œë¡œ ì„ í˜¸ ì„±í–¥ê³¼ ê´€ë ¨ëœ íƒœê·¸ë§Œ ì¶”ì¶œ
#     for axis in PERSONA_AXES:
#         if user_persona_vector[axis] > 0.3: # ì‚¬ìš©ìê°€ ê°•í•˜ê²Œ ì„ í˜¸í•˜ëŠ” ì„±í–¥
#             # generator_bert.pyì— ì •ì˜ëœ íƒœê·¸ í›„ë³´ë¥¼ ì„ì‹œë¡œ ì‚¬ìš©
#             TAG_CANDIDATES = {
#                 'narrative': ["#ê°“ì„œì‚¬", "#ìŠ¤í† ë¦¬_ëª°ì…", "#ê°ë™ì "],
#                 'freedom': ["#ë†’ì€_ììœ ë„", "#íƒí—˜", "#ë‚˜ë§Œì˜_ì„ íƒ"],
#                 'stability': ["#ê°“ì í™”", "#ë²„ê·¸ì—†ìŒ", "#ì¾Œì í•¨"],
#                 'challenge': ["#í•µì‹¬_ë‚œì´ë„", "#ë„ì „ì˜ì‹", "#í”¼ì§€ì»¬_ê²Œì„"]
#             }
#             all_keywords.extend(TAG_CANDIDATES.get(axis, []))

#     return top_reviews_text, list(set(all_keywords)) # ì¤‘ë³µ ì œê±°

# Streamlit ì•± ë ˆì´ì•„ì›ƒ
def main_app():
    st.set_page_config(page_title="ê²Œì„ ë¦¬ë·° ì„±í–¥ ë¶„ì„ê¸°", layout="wide")
    st.title("ğŸ® ê²Œì„ ë¦¬ë·° í†µí•© ë¶„ì„ íŒŒì´í”„ë¼ì¸")
    st.markdown("---")

    # BERT ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ë° ìºì‹±
    tokenizer, model = load_bert_resources()
    if tokenizer is None or model is None:
        st.warning("BERT ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        st.stop() 

    # 1. ì‹ ê·œ ê²Œì„ í¬ë¡¤ë§ ë° ë¶„ì„
    st.header("1. ì‹ ê·œ ê²Œì„ í¬ë¡¤ë§ ë° ë¶„ì„")
    search_term = st.text_input("ë¶„ì„í•  **ê²Œì„ ì´ë¦„**ì„ ì…ë ¥í•˜ê³  [ê²€ìƒ‰] ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”:", key="search_term")
    review_limit = st.slider("ìˆ˜ì§‘í•  ë¦¬ë·° ê°œìˆ˜ (ìµœëŒ€ 500ê°œ)", 50, 500, 200, step=50)
    
    if st.button("ğŸ” ê²Œì„ ê²€ìƒ‰"):
        if not search_term:
            st.warning("ê²€ìƒ‰í•  ê²Œì„ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.session_state['search_results'] = []
            return
            
        with st.spinner(f"'{search_term}' ê²€ìƒ‰ ì¤‘..."):
            results = search_games(search_term) # ğŸ’¡ ìƒˆë¡œìš´ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
            
        if not results:
            st.error(f"âŒ '{search_term}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state['search_results'] = []
        else:
            st.session_state['search_results'] = results
            st.success(f"âœ… ì´ {len(results)}ê°œì˜ ì—°ê´€ ê²Œì„ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ëª©ë¡ì—ì„œ ê²Œì„ì„ ì„ íƒí•˜ì„¸ìš”.")
            
    if 'search_results' in st.session_state and st.session_state['search_results']:
        st.subheader("ê²€ìƒ‰ ê²°ê³¼ ë° ë¶„ì„í•  ê²Œì„ ì„ íƒ")
        # Streamlitì˜ columnsë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ í‘œì‹œ
        
        # ì„ íƒëœ ê²Œì„ ì •ë³´ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        selected_game_info = None 
        
        # ê° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¹´ë“œ í˜•íƒœë¡œ í‘œì‹œ
        for i, item in enumerate(st.session_state['search_results']):
            col1, col2 = st.columns([1, 4])
            
            with col1:
                # ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (Noneì´ ì•„ë‹ˆê³  ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ ë•Œ)
                if item.get('header_image'):
                    st.image(item['header_image'], width=100, caption=str(item['app_id']))
                else:
                    # ì´ë¯¸ì§€ê°€ ì—†ì„ ê²½ìš° ëŒ€ì²´ í…ìŠ¤íŠ¸ë‚˜ ë¹ˆ ê³µê°„ í‘œì‹œ
                    st.write("ğŸ–¼ï¸ (No Image)")
            
            with col2:
                st.markdown(f"{item['name']}")
                st.markdown(
                    f"""
                    - **App ID:** {item['app_id']}
                    """
                )
                # ë¶„ì„ ì‹œì‘ ë²„íŠ¼ ì¶”ê°€ (ê° ì¹´ë“œë³„ë¡œ ë²„íŠ¼ì´ ìƒê¹€)
                if st.button(f"ğŸš€ {item['name']} ë¶„ì„ ì‹œì‘", key=f"analyze_btn_{item['app_id']}"):
                    selected_game_info = item
                    break # ë²„íŠ¼ì´ ëˆŒë¦¬ë©´ ë£¨í”„ ì¢…ë£Œ
                
        if selected_game_info:
            new_game_name = selected_game_info['name']
            app_id = selected_game_info['app_id']
            
            with st.spinner(f"ê²Œì„ '{new_game_name}' ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ (ID: {app_id})..."):                
                # a. í¬ë¡¤ë§ (run_collection í˜¸ì¶œ - ì´ì œ App IDë¥¼ ì§ì ‘ ì „ë‹¬)
                json_path, app_id, error = run_collection(app_id, new_game_name, limit=review_limit)
                if error: st.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {error}"); return
                st.success(f"âœ… App ID ë°œê²¬: {app_id}")
                
                # b. ì„±í–¥ ë¶„ì„ (analyzer.py í˜¸ì¶œ)
                analyzed_path, error = run_analysis(json_path, app_id, new_game_name)
                if error: st.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {error}"); return
                st.success(f"âœ… ì„±í–¥ ë²¡í„° ë¶„ì„ ì™„ë£Œ")
                
                # c. BERT ìƒì„± (generator_bert.py í˜¸ì¶œ)
                summary, tags, output_path, pos_ratio = run_bert_generation(analyzed_path, new_game_name, tokenizer, model)

                st.success(f"âœ… íŒŒì´í”„ë¼ì¸ ë¶„ì„ ì™„ë£Œ! ê¸ì • ë¹„ìœ¨: {pos_ratio}%, ìš”ì•½: {summary[:50]}...")
                st.balloons()
                st.session_state['last_analyzed_game'] = new_game_name
                st.session_state['search_results'] = [] # ë¶„ì„ ì™„ë£Œ í›„ ê²€ìƒ‰ ê²°ê³¼ ì´ˆê¸°í™”
                st.rerun()
                
    st.markdown("---")
    
    # 2. ë¶„ì„ëœ ê²Œì„ ì„ íƒ ë° ê°œì¸í™” ë¶„ì„ ì„¹ì…˜
    st.header("2. ë¶„ì„ëœ ê²Œì„ ì„ íƒ ë° ê°œì¸í™”")

    try:
        analyzed_files = [f for f in os.listdir(DATA_DIR) if f.startswith('analyzed_') and f.endswith('.csv')]
        game_options = {}
        for f in analyzed_files:
            base_name = f.replace('.csv', '').replace('_reviews', '')
            parts = base_name.split('_', 2)
            
            if len(parts) == 3:
                game_name_with_underscores = parts[2]
                game_name = game_name_with_underscores.replace('_', ' ') # ê³µë°±ìœ¼ë¡œ ë³€í™˜
            else:
                # íŒŒì¼ ì´ë¦„ í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¥¼ ê²½ìš° í´ë°±
                game_name = f"Unknown Game ({f})" 
            
            game_options[game_name] = f
            
        available_games = list(game_options.keys())

    except FileNotFoundError:
        available_games = []

    if not available_games:
        st.warning("ë¶„ì„ëœ ê²Œì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. 1ë²ˆì—ì„œ ìƒˆ ê²Œì„ì„ ë¶„ì„í•˜ì„¸ìš”.")
        return
        
    last_game_name = st.session_state.get('last_analyzed_game')

    if last_game_name and last_game_name in available_games:
        default_index = available_games.index(last_game_name)
    else:
        # ğŸ’¡ available_games ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸ í›„ 0ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        default_index = 0
        
    game_name_select = st.selectbox("ê°œì¸í™” ë¶„ì„í•  ê²Œì„ì„ ì„ íƒí•˜ì„¸ìš”:", available_games, index=default_index)
    
    # ë°ì´í„° ë¡œë“œ
    selected_csv_file = game_options[game_name_select]
    df = pd.read_csv(os.path.join(DATA_DIR, selected_csv_file))
    
    base_csv_name_no_suffix = selected_csv_file.replace('.csv', '').replace('_reviews', '')
    
    try:
        safe_game_name_with_underscore = base_csv_name_no_suffix.split('_', 2)[2] 
    except IndexError:
        st.error("ë¶„ì„ íŒŒì¼ ì´ë¦„ íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ëª… í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        return

    # BERT ë¶„ì„ ê²°ê³¼ ë¡œë“œ (TXT íŒŒì¼)
    txt_filename = f"BERT_Analysis_{safe_game_name_with_underscore}.txt"
    txt_path = os.path.join(DATA_DIR, txt_filename)

    summary_data = load_summary_data(txt_path) 

    bert_pos_ratio = summary_data['positive_ratio'] * 100.0 if summary_data['positive_ratio'] is not None else None
    bert_summary = summary_data['summary']
    bert_tags = summary_data['tags']
    # bert_persona_vector = summary_data['persona_vector']

    # --- 3. ì‚¬ìš©ì ì„±í–¥ ì…ë ¥ ---
    st.header(f"ğŸ‘¤ ì‚¬ìš©ì ë§ì¶¤í˜• ë¶„ì„ ({game_name_select})")
    
    user_persona_input = {}
    cols = st.columns(4)
    
    for i, axis in enumerate(PERSONA_AXES):
        with cols[i]:
            user_persona_input[axis] = st.slider(
                label=PERSONA_LABELS_KO[axis],
                min_value=0,
                max_value=10,
                value=5,
                key=f"user_slider_{axis}"
            )
            
    if st.button(f"âœ¨ '{game_name_select}' ê°œì¸í™” ë¶„ì„ ì‹¤í–‰"):
        
        st.markdown("---")
        
        # ì „ì²´ ë¦¬ë·° ê¸°ë°˜ ê¸ì •/ë¶€ì • ì§€í‘œ í‘œì‹œ
        st.subheader("ğŸ‘ ê²Œì„ ì „ë°˜ì ì¸ ê¸ì •/ë¶€ì • ì§€í‘œ")
        if bert_pos_ratio is not None:
            st.metric(
                label=f"ì „ì²´ ê¸ì • ë¹„ìœ¨ ({len(df)} ë¦¬ë·° ê¸°ì¤€)", 
                value=f"{bert_pos_ratio}%",
                delta="ì¢‹ì•„ìš”! ì´ ê²Œì„ì€ í‰ê· ì ìœ¼ë¡œ ê¸ì •ì ì¸ í‰ê°€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤." if bert_pos_ratio >= 70 else "ì°¸ê³ í•˜ì„¸ìš”. ë¦¬ë·°ê°€ ê¸ì •/ë¶€ì •ìœ¼ë¡œ ë‚˜ë‰˜ê³  ìˆìŠµë‹ˆë‹¤.",
            )
        else:
            st.warning("ì „ì²´ ê¸ì • ë¹„ìœ¨ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        # ì „ì²´ ìš”ì•½
        st.subheader("ğŸ“ ì „ì²´ ë¦¬ë·° ê¸°ë°˜ ê²Œì„ ìš”ì•½ (BERT)")
        st.info(bert_summary)
        
        st.subheader("ğŸ”‘ ì „ì²´ ë¦¬ë·° ê¸°ë°˜ ì¶”ì²œ íƒœê·¸")
        tag_display_bert = " ".join([f'<span style="background-color:#5cb85c; color:white; padding:5px 10px; border-radius:15px; margin-right:5px; font-weight:bold;">{tag}</span>' for tag in bert_tags])
        if bert_tags: # íƒœê·¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì§€ ì•Šìœ¼ë©´ í‘œì‹œ
            st.markdown(tag_display_bert, unsafe_allow_html=True)
        else: # ğŸ’¡ íƒœê·¸ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆë‹¤ë©´ í´ë°± ë©”ì‹œì§€ ì¶œë ¥
            st.warning("ë¶„ì„ ê²°ê³¼, ë‘ë“œëŸ¬ì§€ëŠ” ì‚¬ìš©ì ì„±í–¥(ì„ê³„ê°’ 0.15 ì´ìƒ)ì„ ì°¾ì§€ ëª»í•˜ì—¬ ìƒˆë¡œìš´ íƒœê·¸ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        st.markdown("---")
        
        # ê°œì¸í™” ë¶„ì„ ì‹¤í–‰
        user_vector_dict = {axis: user_persona_input[axis] for axis in PERSONA_AXES}
        top_reviews_text, personalized_tags, pos_count, neg_count = get_personalized_recommendation(df, user_vector_dict)
        
        st.subheader("ğŸ’¡ ì‚¬ìš©ì ë§ì¶¤í˜• ì¶”ì²œ íƒœê·¸")
        tag_display_personal = " ".join([f'<span style="background-color:#f0ad4e; color:white; padding:5px 10px; border-radius:15px; margin-right:5px; font-weight:bold;">{tag}</span>' for tag in personalized_tags])
        if personalized_tags:
            st.markdown(tag_display_personal, unsafe_allow_html=True)
        else:
            st.warning("ì‚¬ìš©ìë‹˜ê³¼ ê°•í•˜ê²Œ ì¼ì¹˜í•˜ëŠ” ë§ì¶¤í˜• íƒœê·¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ í˜¸ë„ ìŠ¬ë¼ì´ë”ë¥¼ 7ì  ì´ìƒìœ¼ë¡œ ì„¤ì •í•´ ë³´ì„¸ìš”.")
        
        st.subheader("ğŸ“– ê°œì¸í™” ìš”ì•½ ë° ì¶”ì²œ ë¦¬ë·°")
        
        st.markdown(f"**ì‚¬ìš©ìë‹˜ê³¼ ìœ ì‚¬í•œ ë¦¬ë·°ì–´ {pos_count + neg_count}ëª…ì˜ ê¸/ë¶€ì • ë¹„ìœ¨:**")
        col_pos, col_neg = st.columns(2)
        col_pos.metric("ğŸ‘ ê¸ì • ë¦¬ë·°", pos_count)
        col_neg.metric("ğŸ‘ ë¶€ì • ë¦¬ë·°", neg_count)
        st.markdown("---")
        
        strong_prefs = [PERSONA_LABELS_KO[k] for k, v in user_vector_dict.items() if v >= 7]
        
        if strong_prefs:
            st.write(f"ì‚¬ìš©ìë‹˜ê³¼ ì·¨í–¥(ê°•ë ¥ ì„ í˜¸: **{', '.join(strong_prefs)}**)ì´ ë¹„ìŠ·í•œ ë¦¬ë·°ì–´ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")
        else:
            st.write("ì‚¬ìš©ìë‹˜ê³¼ ì·¨í–¥ì´ ë¹„ìŠ·í•œ ìƒìœ„ ë¦¬ë·°ë“¤ì˜ í•µì‹¬ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:")

        st.code(top_reviews_text[:1200] + "..." if len(top_reviews_text) > 1200 else top_reviews_text, language='text')


if __name__ == "__main__":
    os.makedirs(DATA_DIR, exist_ok=True)
    main_app()